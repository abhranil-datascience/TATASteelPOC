{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TataSteelPOC_4HrsChallenge.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhranil-datascience/TATASteelPOC/blob/master/TataSteelPOC_4HrsChallenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f0yQyLtvc0Z",
        "colab_type": "code",
        "outputId": "cc345092-f93a-4de5-eb2d-4a753b9de298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "######################### Suppress Warnings #######################################\n",
        "\n",
        "#import warnings\n",
        "#warnings.filterwarnings('ignore')\n",
        "\n",
        "############################## Mount Drive ########################################\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "############################## Change Directory ###################################\n",
        "\n",
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/TCS/TATASteelInductionPOC/')\n",
        "\n",
        "################# Root Import Statements #####################\n",
        "\n",
        "import pandas as pd\n",
        "import pandas_profiling\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import operator\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import metrics as mt\n",
        "import statistics as st\n",
        "\n",
        "########################## Steps ############################# \n",
        "\n",
        "##1. Import Dataset\n",
        "dataframe=pd.read_excel('TATASteelDataset.xlsm', 'Dataset')\n",
        "\n",
        "##2. Understand Dataset features\n",
        "pandas_profiling.ProfileReport(dataframe)\n",
        "\n",
        "\"\"\" Profile report shows 2 warnings: \n",
        "a. Y has 134 zero values (Confirmed By Nitish that 0 is invalid value)\"\"\"\n",
        "dataframe=dataframe[dataframe.Y != 0]\n",
        "\"\"\"\n",
        "b. X11 has 609 zero values (This is ok)\n",
        "c. X2 is categorical so needs to be one  hot encoded\"\"\"\n",
        "dataframe[dataframe.X2 == \"REVERSAL\"]=1\n",
        "\n",
        "\"\"\"\n",
        "e. X1 is boolean\n",
        "f. Rest 20 columns (ID column excluded) is numerical\"\"\"\n",
        "\n",
        "## 3. Check Correlation with Target Variable\n",
        "corr = dataframe.corr()\n",
        "sns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns)\n",
        "plt.show()\n",
        "\"\"\"\n",
        "Based on heat map X1 and X20 have very little impact on Y so removing this column\n",
        "\"\"\"\n",
        "drop_list=['X1','X20']\n",
        "dataframe=dataframe.drop(drop_list, axis=1)\n",
        "\n",
        "## 4. Convert to Numpy Array\n",
        "X=dataframe.iloc[:,1:20].values\n",
        "Y=dataframe.iloc[:,20].values\n",
        "\n",
        "## 5. Scale Dataset\n",
        "XScaler=StandardScaler()\n",
        "YScaler=StandardScaler()\n",
        "XScaled=XScaler.fit_transform(X)\n",
        "YScaled=YScaler.fit_transform(Y.reshape(-1,1))\n",
        "\n",
        "## 6. Train Test Split\n",
        "XTrain,XTest,YTrain,YTest=train_test_split(XScaled,YScaled,test_size=0.1,random_state=42)\n",
        "\n",
        "## 7. Check if dataset is linear or non linear\n",
        "RegressorSVR=SVR()\n",
        "parameters=[{'kernel':['linear','rbf','poly','sigmoid']}]\n",
        "GS=GridSearchCV(estimator=RegressorSVR,param_grid=parameters,scoring='neg_mean_squared_error',cv=9)\n",
        "GS.fit(XTrain,YTrain)\n",
        "best_params=GS.best_params_#linear\n",
        "\n",
        "\n",
        "## 8. Verify Feature Dependencies\n",
        "RegressorRF=RandomForestRegressor()\n",
        "RegressorRF.fit(XTrain,YTrain)\n",
        "imp=RegressorRF.feature_importances_\n",
        "Importances={}\n",
        "for indexes in range(0,len(imp)):\n",
        "  Importances[indexes]=imp[indexes]\n",
        "sorted_x = sorted(Importances.items(), key=operator.itemgetter(1))\n",
        "\"\"\"\n",
        "Based on the above feature importances we need to drop column number 4,8,9,10,13,18 as their score is  < 0.01\n",
        "\"\"\"\n",
        "XTrain=XTrain[:,[0,1,2,3,5,6,7,11,12,14,15,16,17]]\n",
        "XTest=XTest[:,[0,1,2,3,5,6,7,11,12,14,15,16,17]]\n",
        "\n",
        "## 9. Hypertune RandomForest Regressor\n",
        "RegressorHyp=RandomForestRegressor()\n",
        "parametersRF=[{'n_estimators':[5,6,7,8,9,10,11,12,13,14,15]}]\n",
        "GS=GridSearchCV(estimator=RegressorHyp,param_grid=parametersRF,scoring='r2',cv=10)\n",
        "GS.fit(XTrain,YTrain)\n",
        "best_params_RF=GS.best_params_#n_estimators=13\n",
        "\n",
        "\"\"\" We can do more hypertuning of parameter but due to lack of time i am limiting this to only n_estimators\"\"\"\n",
        "\n",
        "## 10. Create Regressor and ApplyKFold\n",
        "regressor=RandomForestRegressor(n_estimators=13)\n",
        "r2=[]\n",
        "kfold=KFold(n_splits=9,shuffle=True)\n",
        "for count in range(0,100):\n",
        "  for train_idx,val_idx in kfold.split(XTrain):\n",
        "    XTrainKF_Train,XTrainKF_Val=XTrain[train_idx],XTrain[val_idx]\n",
        "    YTrainKF_Train,YTrainKF_Val=YTrain[train_idx],YTrain[val_idx]\n",
        "    regressor.fit(XTrainKF_Train,YTrainKF_Train)\n",
        "    YPred=regressor.predict(XTrainKF_Val)\n",
        "    r2.append(mt.r2_score(YTrainKF_Val,YPred))\n",
        "print(\"Goodness Of Fit in Training Set: \"+str(st.mean(r2)))#Goodness Of Fit: 0.8244056285284584\n",
        "\n",
        "## 11. Predict Test Data\n",
        "FinalRegressor=regressor.fit(XTrain,YTrain)\n",
        "TestPred=FinalRegressor.predict(XTest)\n",
        "FinalPrediction=YScaler.inverse_transform(TestPred)\n",
        "GOF=mt.r2_score(YScaler.inverse_transform(YTest),FinalPrediction)\n",
        "print(\"Goodness Of Fit in Test Set: \"+str(GOF))#Goodness Of Fit: 0.841222269015523"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Goodness Of Fit in Test Set: 0.838557040754645\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}